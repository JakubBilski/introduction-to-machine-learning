{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating workload and task duration in a Machine Learning project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... is very difficult. Let's say we have a clear task at hand that can be achieved using Machine Learning.\n",
    "##### Data engineering:\n",
    "- is the data ready at-hand, or does it need to be transported/extracted from external sources? Do we have to wait for it?\n",
    "- on-premises: how much processing power do we have? Cloud: how much money can we spend?\n",
    "- do we need any changes to the infrastructure?\n",
    "- how much effort and costs will be necessary to deploy the solution? What's the end goal - a python script saving the results to a data lake, or a web service with an API and thousands of users?\n",
    "##### Data quality:\n",
    "- do we have enough data to achieve the desired performance?\n",
    "- is manual labeling necessary?\n",
    "- can we train on the original data, or does it have to be anonymized?\n",
    "##### Team members:\n",
    "- what knowledge and experience do we have as a team?\n",
    "- lack of experience = higher chances of an inaccurate estimation\n",
    "##### Machine learning problem:\n",
    "- do we know what's the level of performance we should be able to get?\n",
    "- are we going to use a single model, or compare the performance of different models?\n",
    "- how difficult will it be to develop the code, given our architecture? What are the bottlenecks? (The data could be hosted in a relational database. Is could be possible to load all of the data into RAM. Can we run something in parallell?)\n",
    "##### Levels of complexity:\n",
    "1. The task is extremely typical. An off-the-shelf pre-trained model can be used\n",
    "2. It's possible to transform the data (select predictors, run metrics) so an off-the-shelf pre-trained model can be used\n",
    "3. We can train/fine-tune an off-the-shelf model\n",
    "4. The task is not typical OR we want to create a proprietary model. No existing solution can be applied. A brand new model will be designed\n",
    "##### Spike:\n",
    "- we don't have the knowledge to estimate some of the bullet points above. How much time do we need to do that?\n",
    "- what's the risk that the project turns out to be worthless along the way?\n",
    "\n",
    "After the estimation, a business perspective should be considered:\n",
    "- what's the value that the model generates? Is it even useful?\n",
    "- given the data we have, can we do something else?\n",
    "\n",
    "Suprisingly often, the client just has some money to spend on a machine learning model, and the goal of such a model can be up for negotiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/estimating_workload/frame140760.jpg\" alt=\"drawing\" width=\"1500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's say we want to detect shot types in movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plots/estimating_workload/types_of_shots.webp\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have an annotated dataset of four films, one frame per second.\n",
    "\n",
    "How should we approach this problem?\n",
    "\n",
    "Certainly not by learning a CNN from scratch. Let's try and see why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRY_RUN = True  # run without data, show example results from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import bokeh, bokeh.plotting, bokeh.palettes, bokeh.transform\n",
    "import ipywidgets\n",
    "import IPython.display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision, torchvision.transforms, torchvision.io, torch.utils.data, torchvision.datasets, torchvision.models\n",
    "import time\n",
    "\n",
    "from facenet_pytorch import MTCNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"d957fe12-8b7d-4136-a428-54789f5e5353\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"d957fe12-8b7d-4136-a428-54789f5e5353\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.3.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.3.0.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"d957fe12-8b7d-4136-a428-54789f5e5353\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bokeh.plotting.output_notebook()\n",
    "bokeh.io.output_file('plots/estimating_workload/temp_bokeh.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore the case when there are no people in the shot for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_shot = ['Closeup', 'Medium closeup', 'Medium shot', 'Cowboy shot', 'Full shot', 'Long shot']\n",
    "shot_to_int = {int_to_shot[i]: i for i in range(len(int_to_shot))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "  <head>\n",
       "    <meta charset=\"utf-8\">\n",
       "    <title>Bokeh Plot</title>\n",
       "    <style>\n",
       "      html, body {\n",
       "        box-sizing: border-box;\n",
       "        display: flow-root;\n",
       "        height: 100%;\n",
       "        margin: 0;\n",
       "        padding: 0;\n",
       "      }\n",
       "    </style>\n",
       "    <script type=\"text/javascript\" src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.0.min.js\"></script>\n",
       "    <script type=\"text/javascript\">\n",
       "        Bokeh.set_log_level(\"info\");\n",
       "    </script>\n",
       "  </head>\n",
       "  <body>\n",
       "    <div id=\"ee1c1573-5ab1-4a42-a820-492953da792c\" data-root-id=\"p1340\" style=\"display: contents;\"></div>\n",
       "  \n",
       "    <script type=\"application/json\" id=\"p1354\">\n",
       "      {\"d06d7779-94c4-40f9-a52b-613b504bb7ae\":{\"version\":\"3.3.0\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"GridPlot\",\"id\":\"p1340\",\"attributes\":{\"rows\":null,\"cols\":null,\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1339\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"ToolProxy\",\"id\":\"p1338\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1201\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"@shottype: @value\"}},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1241\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"@shottype: @value\"}},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1281\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"@shottype: @value\"}},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1321\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"@shottype: @value\"}}]}}]}},\"children\":[[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1178\",\"attributes\":{\"height\":350,\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1188\",\"attributes\":{\"start\":-0.5}},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1180\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1189\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1190\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1181\",\"attributes\":{\"text\":\"Drive my car\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1213\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1204\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1205\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1206\"},\"data\":{\"type\":\"map\",\"entries\":[[\"index\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAEAAAACAAAAAwAAAAQAAAAFAAAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"shottype\",{\"type\":\"ndarray\",\"array\":[\"Closeup\",\"Medium closeup\",\"Medium shot\",\"Cowboy shot\",\"Full shot\",\"Long shot\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}],[\"value\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"vgcAAMEGAAD9BgAADgIAAHgCAAAxBAAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"angle\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"J8JsAO3F+T8DKr1NtXv2PxRNZpBxQ/c/MEVe7g1c2z88Wh5Jwm/gPyJEboPe5+s/\"},\"shape\":[6],\"dtype\":\"float64\",\"order\":\"little\"}],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#aec7e8\",\"#ff7f0e\",\"#ffbb78\",\"#2ca02c\",\"#98df8a\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1214\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1215\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1210\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1202\",\"attributes\":{\"field\":\"angle\",\"include_zero\":true}}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1203\",\"attributes\":{\"field\":\"angle\"}}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1211\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1202\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1203\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1212\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1202\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1203\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1187\",\"attributes\":{\"tools\":[{\"id\":\"p1201\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1196\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1197\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1198\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1199\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1191\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1192\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1193\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1194\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1195\",\"attributes\":{\"axis\":{\"id\":\"p1191\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1200\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1196\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1216\",\"attributes\":{\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1217\",\"attributes\":{\"label\":{\"type\":\"field\",\"field\":\"shottype\"},\"renderers\":[{\"id\":\"p1213\"}]}}]}}]}},0,0],[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1218\",\"attributes\":{\"height\":350,\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1228\",\"attributes\":{\"start\":-0.5}},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1220\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1229\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1230\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1221\",\"attributes\":{\"text\":\"Once Upon A Time ... In Hollywood\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1253\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1244\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1245\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1246\"},\"data\":{\"type\":\"map\",\"entries\":[[\"index\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAEAAAACAAAAAwAAAAQAAAAFAAAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"shottype\",{\"type\":\"ndarray\",\"array\":[\"Closeup\",\"Medium closeup\",\"Medium shot\",\"Cowboy shot\",\"Full shot\",\"Long shot\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}],[\"value\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"EgUAAJcEAADzBgAA/QEAAOMBAAAZBAAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"angle\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"G8egxk+89D/jd0EwSsXyP+zj0slpa/w/NVFupjhD4D97LVyRH93eP9XCKsj/wfA/\"},\"shape\":[6],\"dtype\":\"float64\",\"order\":\"little\"}],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#aec7e8\",\"#ff7f0e\",\"#ffbb78\",\"#2ca02c\",\"#98df8a\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1254\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1255\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1250\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1242\",\"attributes\":{\"field\":\"angle\",\"include_zero\":true}}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1243\",\"attributes\":{\"field\":\"angle\"}}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1251\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1242\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1243\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1252\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1242\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1243\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1227\",\"attributes\":{\"tools\":[{\"id\":\"p1241\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1236\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1237\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1238\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1239\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1231\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1232\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1233\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1234\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1235\",\"attributes\":{\"axis\":{\"id\":\"p1231\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1240\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1236\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1256\",\"attributes\":{\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1257\",\"attributes\":{\"label\":{\"type\":\"field\",\"field\":\"shottype\"},\"renderers\":[{\"id\":\"p1253\"}]}}]}}]}},0,1],[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1258\",\"attributes\":{\"height\":350,\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1268\",\"attributes\":{\"start\":-0.5}},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1260\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1269\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1270\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1261\",\"attributes\":{\"text\":\"Butch Cassidy And The Sundance Kid\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1293\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1284\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1285\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1286\"},\"data\":{\"type\":\"map\",\"entries\":[[\"index\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAEAAAACAAAAAwAAAAQAAAAFAAAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"shottype\",{\"type\":\"ndarray\",\"array\":[\"Closeup\",\"Medium closeup\",\"Medium shot\",\"Cowboy shot\",\"Full shot\",\"Long shot\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}],[\"value\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"agQAAMACAAAPAwAArQAAAOwAAABtAQAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"angle\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"z3JyeQ3AAECPY2BO/d70Py/AcdKONvc/IXlnceqD1D8LtNKRdvzbPyQK0ddQpOU/\"},\"shape\":[6],\"dtype\":\"float64\",\"order\":\"little\"}],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#aec7e8\",\"#ff7f0e\",\"#ffbb78\",\"#2ca02c\",\"#98df8a\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1294\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1295\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1290\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1282\",\"attributes\":{\"field\":\"angle\",\"include_zero\":true}}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1283\",\"attributes\":{\"field\":\"angle\"}}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1291\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1282\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1283\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1292\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1282\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1283\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1267\",\"attributes\":{\"tools\":[{\"id\":\"p1281\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1276\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1277\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1278\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1279\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1271\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1272\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1273\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1274\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1275\",\"attributes\":{\"axis\":{\"id\":\"p1271\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1280\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1276\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1296\",\"attributes\":{\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1297\",\"attributes\":{\"label\":{\"type\":\"field\",\"field\":\"shottype\"},\"renderers\":[{\"id\":\"p1293\"}]}}]}}]}},1,0],[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1298\",\"attributes\":{\"height\":350,\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1308\",\"attributes\":{\"start\":-0.5}},\"y_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1300\"},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1309\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1310\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1301\",\"attributes\":{\"text\":\"Blade Runner (1982)\"}},\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1333\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1324\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1325\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1326\"},\"data\":{\"type\":\"map\",\"entries\":[[\"index\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"AAAAAAEAAAACAAAAAwAAAAQAAAAFAAAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"shottype\",{\"type\":\"ndarray\",\"array\":[\"Closeup\",\"Medium closeup\",\"Medium shot\",\"Cowboy shot\",\"Full shot\",\"Long shot\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}],[\"value\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"xwYAAIkBAABBAQAAXAAAAGgAAABlAAAA\"},\"shape\":[6],\"dtype\":\"int32\",\"order\":\"little\"}],[\"angle\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"p6TtkFXCD0CnIYUrgcbsP+JffHvqgOc/CYmevOXxyj84Ck+Hn3XOP+zpohSxlM0/\"},\"shape\":[6],\"dtype\":\"float64\",\"order\":\"little\"}],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#aec7e8\",\"#ff7f0e\",\"#ffbb78\",\"#2ca02c\",\"#98df8a\"],\"shape\":[6],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1334\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1335\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1330\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1322\",\"attributes\":{\"field\":\"angle\",\"include_zero\":true}}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"type\":\"object\",\"name\":\"CumSum\",\"id\":\"p1323\",\"attributes\":{\"field\":\"angle\"}}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1331\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1322\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1323\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Wedge\",\"id\":\"p1332\",\"attributes\":{\"x\":{\"type\":\"value\",\"value\":0},\"y\":{\"type\":\"value\",\"value\":1},\"radius\":{\"type\":\"value\",\"value\":0.4},\"start_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1322\"}},\"end_angle\":{\"type\":\"expr\",\"expr\":{\"id\":\"p1323\"}},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1307\",\"attributes\":{\"tools\":[{\"id\":\"p1321\"}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1316\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1317\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1318\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1319\"}}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1311\",\"attributes\":{\"visible\":false,\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1312\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1313\"},\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1314\"}}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1315\",\"attributes\":{\"axis\":{\"id\":\"p1311\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1320\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1316\"},\"grid_line_color\":null}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1336\",\"attributes\":{\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1337\",\"attributes\":{\"label\":{\"type\":\"field\",\"field\":\"shottype\"},\"renderers\":[{\"id\":\"p1333\"}]}}]}}]}},1,1]]}}]}}\n",
       "    </script>\n",
       "    <script type=\"text/javascript\">\n",
       "      (function() {\n",
       "        const fn = function() {\n",
       "          Bokeh.safely(function() {\n",
       "            (function(root) {\n",
       "              function embed_document(root) {\n",
       "              const docs_json = document.getElementById('p1354').textContent;\n",
       "              const render_items = [{\"docid\":\"d06d7779-94c4-40f9-a52b-613b504bb7ae\",\"roots\":{\"p1340\":\"ee1c1573-5ab1-4a42-a820-492953da792c\"},\"root_ids\":[\"p1340\"]}];\n",
       "              root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "              }\n",
       "              if (root.Bokeh !== undefined) {\n",
       "                embed_document(root);\n",
       "              } else {\n",
       "                let attempts = 0;\n",
       "                const timer = setInterval(function(root) {\n",
       "                  if (root.Bokeh !== undefined) {\n",
       "                    clearInterval(timer);\n",
       "                    embed_document(root);\n",
       "                  } else {\n",
       "                    attempts++;\n",
       "                    if (attempts > 100) {\n",
       "                      clearInterval(timer);\n",
       "                      console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "                    }\n",
       "                  }\n",
       "                }, 10, root)\n",
       "              }\n",
       "            })(window);\n",
       "          });\n",
       "        };\n",
       "        if (document.readyState != \"loading\") fn();\n",
       "        else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "      })();\n",
       "    </script>\n",
       "  </body>\n",
       "</html>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_class_distributions(class_labels, title):\n",
    "    labels, counts = np.unique(class_labels, return_counts=True)\n",
    "    X = {l: c for l, c in sorted(zip(labels, counts), key=lambda x: shot_to_int[x[0]])}\n",
    "\n",
    "    data = pd.Series(X).reset_index(name='value').rename(columns={'index': 'shottype'})\n",
    "    data['angle'] = data['value']/data['value'].sum() * 2 * 3.14159\n",
    "    data['color'] = bokeh.palettes.Category20[len(X.keys())]\n",
    "\n",
    "    p = bokeh.plotting.figure(\n",
    "        height=350,\n",
    "        title=title,\n",
    "        toolbar_location=None,\n",
    "        tools=\"hover\",\n",
    "        tooltips=\"@shottype: @value\",\n",
    "        x_range=(-0.5, 1.0)\n",
    "        )\n",
    "\n",
    "    p.wedge(x=0, y=1, radius=0.4,\n",
    "            start_angle=bokeh.transform.cumsum('angle', include_zero=True), end_angle=bokeh.transform.cumsum('angle'),\n",
    "            line_color=\"white\", fill_color='color', legend_field='shottype', source=data)\n",
    "\n",
    "    p.axis.axis_label = None\n",
    "    p.axis.visible = False\n",
    "    p.grid.grid_line_color = None\n",
    "    return p\n",
    "\n",
    "if DRY_RUN:\n",
    "    gridplot = open('./plots/estimating_workload/class_distribution.html', 'r').read()\n",
    "    display(IPython.display.HTML(gridplot))\n",
    "else:\n",
    "    datasources = [\n",
    "        ('DMCpredictions.csv', 'Drive my car'),\n",
    "        ('OUATIHpredictions.csv', 'Once Upon A Time ... In Hollywood'),\n",
    "        ('BCATSKpredictions.csv', 'Butch Cassidy And The Sundance Kid'),\n",
    "        ('BRpredictions.csv', 'Blade Runner (1982)'),\n",
    "    ]\n",
    "    figures = []\n",
    "    for filename, title in datasources:\n",
    "        df = pd.read_csv(filename)\n",
    "        labels = df[df.iloc[:, 1].isin(int_to_shot)].iloc[:, 1]\n",
    "        figures.append(plot_class_distributions(labels, title))\n",
    "    gridplot = bokeh.layouts.gridplot([[figures[0], figures[1]], [figures[2], figures[3]]])\n",
    "    bokeh.plotting.save(gridplot)\n",
    "    gridplot = open('./plots/estimating_workload/temp_bokeh.html', 'r').read()\n",
    "    display(IPython.display.HTML(gridplot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a balanced dataset for each film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomShotTypesBalancedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_labels = self.img_labels[self.img_labels.iloc[:, 1].isin(int_to_shot)]\n",
    "        smallest_class_count = min(np.unique(self.img_labels.iloc[:, 1], return_counts=True)[1])\n",
    "        self.img_labels = self.img_labels.sample(frac=1).groupby(self.img_labels.columns[1]).apply(lambda x: x.sample(n=min(smallest_class_count, len(x))))\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.shot_to_int = shot_to_int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, f'frame{self.img_labels.iloc[idx, 0]}.jpg')\n",
    "        image = PIL.Image.open(img_path)\n",
    "        label = self.shot_to_int[self.img_labels.iloc[idx, 1]]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7092 training samples\n",
      "552 validation samples\n"
     ]
    }
   ],
   "source": [
    "if DRY_RUN:\n",
    "    print(f'7092 training samples')\n",
    "    print(f'552 validation samples')\n",
    "else:\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((64, 64)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    batch_size = 4\n",
    "    dataset_dmc = CustomShotTypesBalancedDataset('DMCpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\DMC', transform=transform)\n",
    "    dataset_ouatih = CustomShotTypesBalancedDataset('OUATIHpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\OUATIH', transform=transform)\n",
    "    dataset_bcatsk = CustomShotTypesBalancedDataset('BCATSKpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\BCATSK', transform=transform)\n",
    "    dataset_br = CustomShotTypesBalancedDataset('BRpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\BR', transform=transform)\n",
    "\n",
    "    trainset = torch.utils.data.ConcatDataset([dataset_dmc, dataset_ouatih, dataset_bcatsk])\n",
    "    valset = dataset_br\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dataloaders = {'train': trainloader, 'val': valloader}\n",
    "    dataset_sizes = {'train': trainset.__len__(), 'val': valset.__len__()}\n",
    "\n",
    "    print(f'{dataset_sizes[\"train\"]} training samples')\n",
    "    print(f'{dataset_sizes[\"val\"]} validation samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./plots/estimating_workload/dataloaders_1.png\" style=\"width:50%\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium shot Medium closeup Medium closeup Medium shot\n"
     ]
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "if DRY_RUN:\n",
    "    display(IPython.display.HTML(\n",
    "        '<img src=\"./plots/estimating_workload/dataloaders_1.png\" style=\"width:50%\">'\n",
    "    ))\n",
    "    print(' '.join(['Medium shot', 'Medium closeup', 'Medium closeup', 'Medium shot']))\n",
    "else:\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = next(dataiter)\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    print(' '.join(f'{int_to_shot[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 13 * 13, 120) # ((64-4)/2-4)/2=13\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleCNN()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "def train_cnn(model, criterion, optimizer, scheduler, num_epochs, best_model_params_path):\n",
    "    history = {'train': [], 'val': []}\n",
    "    since = time.time()\n",
    "    best_model_params_path = 'best_model_params.pt'\n",
    "    torch.save(model.state_dict(), best_model_params_path)\n",
    "    best_acc = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            history[phase].append((epoch_loss, epoch_acc))\n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_params_path))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DRY_RUN:\n",
    "    model, learning_history = train_cnn(model, criterion, optimizer, lr_scheduler, num_epochs=30, best_model_params_path='SimpleCNNBestParams.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting function\n",
    "\n",
    "def plot_learning_history(learning_history):\n",
    "    train_loss_history = [loss for loss, acc in learning_history['train']]\n",
    "    val_loss_history = [loss for loss, acc in learning_history['val']]\n",
    "    train_acc_history = [acc for loss, acc in learning_history['train']]\n",
    "    val_acc_history = [acc for loss, acc in learning_history['val']]\n",
    "\n",
    "    fig1 = bokeh.plotting.figure(\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset\",\n",
    "        width=500,\n",
    "        height=500,\n",
    "        x_axis_label='epoch',\n",
    "        y_axis_label='loss',\n",
    "        title='Custom CNN learning history: loss'\n",
    "    )\n",
    "\n",
    "    fig1.line(list(range(len(train_loss_history))), train_loss_history, line_width=3.5, line_color=\"red\", alpha=1, legend_label='train')\n",
    "    fig1.line(list(range(len(val_loss_history))), val_loss_history, line_width=3.5, line_color=\"blue\", alpha=1, legend_label='val')\n",
    "\n",
    "\n",
    "    fig2 = bokeh.plotting.figure(\n",
    "        tools=\"pan,wheel_zoom,box_zoom,reset\",\n",
    "        width=500,\n",
    "        height=500,\n",
    "        x_axis_label='epoch',\n",
    "        y_axis_label='accuracy',\n",
    "        title='Custom CNN learning history: accuracy'\n",
    "    )\n",
    "\n",
    "    fig2.line(list(range(len(train_acc_history))), train_acc_history, line_width=3.5, line_color=\"red\", alpha=1, legend_label='train')\n",
    "    fig2.line(list(range(len(val_acc_history))), val_acc_history, line_width=3.5, line_color=\"blue\", alpha=1, legend_label='val')\n",
    "\n",
    "    gridplot = bokeh.layouts.gridplot([[fig1, fig2]])\n",
    "    bokeh.plotting.save(gridplot)\n",
    "    gridplot = open('./plots/estimating_workload/temp_bokeh.html', 'r').read()\n",
    "    display(IPython.display.HTML(gridplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./plots/estimating_workload/learninghistory_1.png\" style=\"width:70%\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DRY_RUN:\n",
    "    display(IPython.display.HTML(\n",
    "        '<img src=\"./plots/estimating_workload/learninghistory_1.png\" style=\"width:70%\">'\n",
    "    ))\n",
    "else:\n",
    "    plot_learning_history(learning_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model fails to generalize into the validation dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"row\">\n",
    "  <div class=\"column\">\n",
    "    <img src=\"./plots/estimating_workload/frame21024.jpg\" style=\"width:40%\">\n",
    "    <img src=\"./plots/estimating_workload/frame129528.jpg\" style=\"width:40%\">\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"./plots/estimating_workload/frame80808.jpg\" style=\"width:40%\">\n",
    "    <img src=\"./plots/estimating_workload/frame66864.jpg\" style=\"width:40%\">\n",
    "  </div>\n",
    "  <div class=\"column\">\n",
    "    <img src=\"./plots/estimating_workload/frame64944.jpg\" style=\"width:40%\">\n",
    "    <img src=\"./plots/estimating_workload/frame65616.jpg\" style=\"width:40%\">\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try transfer learning instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7092 training samples\n",
      "552 validation samples\n"
     ]
    }
   ],
   "source": [
    "if DRY_RUN:\n",
    "    print(f'7092 training samples')\n",
    "    print(f'552 validation samples')\n",
    "else:\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    batch_size = 4\n",
    "    dataset_dmc = CustomShotTypesBalancedDataset('DMCpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\DMC', transform=transform)\n",
    "    dataset_ouatih = CustomShotTypesBalancedDataset('OUATIHpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\OUATIH', transform=transform)\n",
    "    dataset_bcatsk = CustomShotTypesBalancedDataset('BCATSKpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\BCATSK', transform=transform)\n",
    "    dataset_br = CustomShotTypesBalancedDataset('BRpredictions.csv', 'C:\\\\Projects\\\\iml\\\\data\\\\films\\\\frames\\\\BR', transform=transform)\n",
    "\n",
    "    trainset = torch.utils.data.ConcatDataset([dataset_dmc, dataset_ouatih, dataset_bcatsk])\n",
    "    valset = dataset_br\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "    valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    dataloaders = {'train': trainloader, 'val': valloader}\n",
    "    dataset_sizes = {'train': trainset.__len__(), 'val': valset.__len__()}\n",
    "\n",
    "    print(f'{dataset_sizes[\"train\"]} training samples')\n",
    "    print(f'{dataset_sizes[\"val\"]} validation samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./plots/estimating_workload/dataloaders_2.png\" style=\"width:50%\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def imshow(inp, title=None):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "if DRY_RUN:\n",
    "    display(IPython.display.HTML(\n",
    "        '<img src=\"./plots/estimating_workload/dataloaders_2.png\" style=\"width:50%\">'\n",
    "    ))\n",
    "else:\n",
    "    inputs, classes = next(iter(dataloaders['train']))\n",
    "    out = torchvision.utils.make_grid(inputs)\n",
    "    imshow(out, title=[int_to_shot[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a pretrained model, freeze the weights and add a single fully connected layer at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(int_to_shot))\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DRY_RUN:\n",
    "    model, learning_history = train_cnn(model, criterion, optimizer, lr_scheduler, num_epochs=30, best_model_params_path='TransferLearningBestParams.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./plots/estimating_workload/learninghistory_2.png\" style=\"width:70%\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if DRY_RUN:\n",
    "    display(IPython.display.HTML(\n",
    "        '<img src=\"./plots/estimating_workload/learninghistory_2.png\" style=\"width:70%\">'\n",
    "    ))\n",
    "else:\n",
    "    plot_learning_history(learning_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better, but still not any good\n",
    "\n",
    "We have an insufficient dataset. Maybe there is a way to avoid training altogether?\n",
    "\n",
    "Let's try to detect faces in frames using a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces(img):\n",
    "    mtcnn = MTCNN(image_size=160, margin=0)\n",
    "    detected_faces_coord, detected_faces_prob = mtcnn.detect(img)\n",
    "    return detected_faces_coord, detected_faces_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll probably need a threshold for face detection. Let's visualize a few examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_samples_for_face_detection = [\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame24432.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame21384.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame27024.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame43104.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame46632.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame131904.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame137232.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame136416.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3377ffbee8174adc8d4bb77225565dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Previous', style=ButtonStyle()), Button(description='Next', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some unimportant interactive plot code here\n",
    "\n",
    "button_previous = ipywidgets.Button(\n",
    "    description='Previous',\n",
    ")\n",
    "button_next = ipywidgets.Button(\n",
    "    description='Next',\n",
    ")\n",
    "slider_threshold = ipywidgets.FloatSlider(\n",
    "    min=0.0, max=1.0, step=0.05, description=\"threshold\", continuous_update=False\n",
    ")\n",
    "selected_image_index = [0]\n",
    "out = ipywidgets.Output()\n",
    "\n",
    "@out.capture(clear_output=True, wait=True)\n",
    "def plot_image_and_detected_faces(change):\n",
    "    IPython.display.clear_output()\n",
    "    out.clear_output()\n",
    "    img = PIL.Image.open(good_samples_for_face_detection[selected_image_index[0]])\n",
    "    detected_faces_coord, detected_faces_prob = detect_faces(img)\n",
    "    img = cv2.imread(good_samples_for_face_detection[selected_image_index[0]])\n",
    "    if detected_faces_coord is not None:\n",
    "        for (x1, y1, x2, y2), prob in zip(detected_faces_coord, detected_faces_prob):\n",
    "            if prob < slider_threshold.value:\n",
    "                continue\n",
    "            cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0))\n",
    "            cv2.putText(img, f\"{prob:.4f}\", (int(x1), int(y1)), 0, 1, (0, 255, 0))\n",
    "\n",
    "    cv2.imwrite('./plots/estimating_workload/cv2outputtemp.jpg', img)\n",
    "    IPython.display.display(IPython.display.Image(filename='./plots/estimating_workload/cv2outputtemp.jpg'))\n",
    "\n",
    "\n",
    "plot_image_and_detected_faces(None)\n",
    "\n",
    "@button_previous.on_click\n",
    "def plot_on_click(b):\n",
    "    selected_image_index[0] -= 1\n",
    "    if selected_image_index[0] == -1:\n",
    "        selected_image_index[0] = len(good_samples_for_face_detection) - 1\n",
    "    plot_image_and_detected_faces(None)\n",
    "\n",
    "@button_next.on_click\n",
    "def plot_on_click(b):\n",
    "    selected_image_index[0] += 1\n",
    "    if selected_image_index[0] == len(good_samples_for_face_detection):\n",
    "        selected_image_index[0] = 0\n",
    "    plot_image_and_detected_faces(None)\n",
    "\n",
    "slider_threshold.observe(plot_image_and_detected_faces, names='value')\n",
    "\n",
    "IPython.display.display(ipywidgets.VBox([ipywidgets.HBox(children=[button_previous, button_next]), slider_threshold, out]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Threshold 0.85 seems good enough. Let's select the biggest face on the image and measure the amount of space below\n",
    "\n",
    "We can use it to roughly estimate the type of the shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.85\n",
    "\n",
    "def predict_shot_type(img_path):\n",
    "    img = PIL.Image.open(img_path)\n",
    "    mtcnn = MTCNN(image_size=1000, margin=0)\n",
    "    detected_faces_coord, detected_faces_prob = mtcnn.detect(img)\n",
    "\n",
    "    if detected_faces_coord is None:\n",
    "        return 'no face detected', None, None, None\n",
    "\n",
    "    biggest_face_coord = None\n",
    "    biggest_face_prob = None\n",
    "    biggest_face_area = 0\n",
    "    for (x1, y1, x2, y2), prob in zip(detected_faces_coord, detected_faces_prob):\n",
    "        if prob < THRESHOLD:\n",
    "            continue\n",
    "        face_area = (x2 - x1) * (y2 - y1)\n",
    "        if face_area > biggest_face_area:\n",
    "            biggest_face_coord = (x1, y1, x2, y2)\n",
    "            biggest_face_prob = prob\n",
    "            biggest_face_area = face_area\n",
    "\n",
    "    if biggest_face_coord is None:\n",
    "        return 'no face detected', None, None, None\n",
    "\n",
    "    (x1, y1, x2, y2) = biggest_face_coord\n",
    "    space_below = img.size[1] - y2\n",
    "    space_below_num_heads = float(space_below) / (y2 - y1)\n",
    "\n",
    "    if space_below_num_heads < 1:\n",
    "        predicted_shot_type = 'Closeup'\n",
    "    elif space_below_num_heads < 2:\n",
    "        predicted_shot_type = 'Medium closeup'\n",
    "    elif space_below_num_heads < 4:\n",
    "        predicted_shot_type = 'Medium shot'\n",
    "    elif space_below_num_heads < 5:\n",
    "        predicted_shot_type = 'Cowboy shot'\n",
    "    elif space_below_num_heads < 7:\n",
    "        predicted_shot_type = 'Full shot'\n",
    "    else:\n",
    "        predicted_shot_type = 'Long shot'\n",
    "    return predicted_shot_type, (x1, y1, x2, y2), biggest_face_prob, space_below_num_heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f09310566d46b6a974efd7a77ae4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Previous', style=ButtonStyle()), Button(description='Next', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some more plot code\n",
    "\n",
    "random_frames = [\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame70368.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame67368.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame83136.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame123144.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame121416.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame144648.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame109752.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BCATSK/frame74328.jpg',\n",
    "\n",
    "]\n",
    "\n",
    "button_previous = ipywidgets.Button(\n",
    "    description='Previous',\n",
    ")\n",
    "button_next = ipywidgets.Button(\n",
    "    description='Next',\n",
    ")\n",
    "selected_image_index = [0]\n",
    "\n",
    "out = ipywidgets.Output()\n",
    "\n",
    "@out.capture(clear_output=True, wait=True)\n",
    "def plot_shot_type_prediction(change):\n",
    "    IPython.display.clear_output()\n",
    "    out.clear_output()\n",
    "    frame_filename = random_frames[selected_image_index[0]]\n",
    "    predicted_shot_type, predicted_coord, prob, space_below_num_heads = predict_shot_type(\n",
    "        frame_filename\n",
    "    )\n",
    "    img = cv2.imread(frame_filename)\n",
    "    if predicted_shot_type != 'no face detected':\n",
    "        (x1, y1, x2, y2) = predicted_coord\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0))\n",
    "        cv2.putText(img, f\"{prob:.4f}\", (int(x1), int(y1)), 0, 1, (0, 255, 0))\n",
    "        for head_space_index in range(int(space_below_num_heads)):\n",
    "            cv2.rectangle(img, (int(x1), int(head_space_index*(y2-y1)+y2+10)), (int(x2), int((head_space_index+1)*(y2-y1)+y2-10)), (0, 0, 255))\n",
    "\n",
    "    cv2.putText(img, predicted_shot_type, (20, 20), 0, 1, (0, 255, 0))\n",
    "    cv2.imwrite('./plots/estimating_workload/cv2outputtemp.jpg', img)\n",
    "    IPython.display.display(IPython.display.Image(filename='./plots/estimating_workload/cv2outputtemp.jpg'))\n",
    "\n",
    "plot_shot_type_prediction(None)\n",
    "\n",
    "@button_previous.on_click\n",
    "def plot_on_click(b):\n",
    "    selected_image_index[0] -= 1\n",
    "    if selected_image_index[0] == -1:\n",
    "        selected_image_index[0] = len(random_frames) - 1\n",
    "    plot_shot_type_prediction(None)\n",
    "\n",
    "@button_next.on_click\n",
    "def plot_on_click(b):\n",
    "    selected_image_index[0] += 1\n",
    "    if selected_image_index[0] == len(random_frames):\n",
    "        selected_image_index[0] = 0\n",
    "    plot_shot_type_prediction(None)\n",
    "\n",
    "IPython.display.display(ipywidgets.VBox([ipywidgets.HBox(children=[button_previous, button_next]), out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the validation set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf31e1907d41430fad23241e19a911bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(description='Previous', style=ButtonStyle()), Button(description='Next', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# some more plot code\n",
    "\n",
    "random_frames = [\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame17688.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame74232.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame153984.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame126528.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame121224.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame53904.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame49704.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame52992.jpg',\n",
    "    'plots/estimating_workload/face_detection_examples/BR/frame84888.jpg',\n",
    "]\n",
    "\n",
    "button_previous = ipywidgets.Button(\n",
    "    description='Previous',\n",
    ")\n",
    "button_next = ipywidgets.Button(\n",
    "    description='Next',\n",
    ")\n",
    "selected_image_index = [0]\n",
    "\n",
    "out = ipywidgets.Output()\n",
    "\n",
    "@out.capture(clear_output=True, wait=True)\n",
    "def plot_shot_type_prediction(change):\n",
    "    IPython.display.clear_output()\n",
    "    out.clear_output()\n",
    "    frame_filename = random_frames[selected_image_index[0]]\n",
    "    predicted_shot_type, predicted_coord, prob, space_below_num_heads = predict_shot_type(\n",
    "        frame_filename\n",
    "    )\n",
    "    img = cv2.imread(frame_filename)\n",
    "    if predicted_shot_type != 'no face detected':\n",
    "        (x1, y1, x2, y2) = predicted_coord\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0))\n",
    "        cv2.putText(img, f\"{prob:.4f}\", (int(x1), int(y1)), 0, 1, (0, 255, 0))\n",
    "        for head_space_index in range(int(space_below_num_heads)):\n",
    "            cv2.rectangle(img, (int(x1), int(head_space_index*(y2-y1)+y2+10)), (int(x2), int((head_space_index+1)*(y2-y1)+y2-10)), (0, 0, 255))\n",
    "\n",
    "    cv2.putText(img, predicted_shot_type, (20, 20), 0, 1, (0, 255, 0))\n",
    "    cv2.imwrite('./plots/estimating_workload/cv2outputtemp.jpg', img)\n",
    "    IPython.display.display(IPython.display.Image(filename='./plots/estimating_workload/cv2outputtemp.jpg'))\n",
    "\n",
    "plot_shot_type_prediction(None)\n",
    "\n",
    "@button_previous.on_click\n",
    "def plot_on_click(b):\n",
    "    selected_image_index[0] -= 1\n",
    "    if selected_image_index[0] == -1:\n",
    "        selected_image_index[0] = len(random_frames) - 1\n",
    "    plot_shot_type_prediction(None)\n",
    "\n",
    "@button_next.on_click\n",
    "def plot_on_click(b):\n",
    "    selected_image_index[0] += 1\n",
    "    if selected_image_index[0] == len(random_frames):\n",
    "        selected_image_index[0] = 0\n",
    "    plot_shot_type_prediction(None)\n",
    "\n",
    "IPython.display.display(ipywidgets.VBox([ipywidgets.HBox(children=[button_previous, button_next]), out]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's by no means a perfect solution, but it works better than everything we did so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two options - gather more data and try transfer learning once more, or use the manual solution. We can make it better by detecting poses instead of faces, adding a moving average to consecutive frames, rotating the images, etc.\n",
    "\n",
    "We went through three levels of complexity mentioned earlier:\n",
    "\n",
    "\n",
    "2. Transforming the data (selecting predictors, running metrics) so an off-the-shelf pre-trained model can be used <font color=\"green\">Face recognition</font>\n",
    "\n",
    "3. The data is typical, but the task is not. We will train/fine-tune an off-the-shelf model <font color=\"green\">Transfer learning</font>\n",
    "\n",
    "4. The task is not typical OR we want to create a proprietary model. No existing solution can be applied. A brand new model will be designed <font color=\"green\">Custom CNN</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imllabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
